{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30bcf562",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6654e14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\salesforce\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "#Tokenizer from scratch on vocabulary of corpus\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "# Decoder\n",
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM # RobertaLM for learning\n",
    "from transformers import RobertaTokenizerFast # After training tokenizern we will wrap it so it can be used by Roberta model\n",
    "\n",
    "\n",
    "#Training\n",
    "# When using previous version of the library you need the following two lines\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2f1a73",
   "metadata": {},
   "source": [
    "# Parameters for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0af58b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 20   # input batch size for training (default: 64)\n",
    "VALID_BATCH_SIZE = 5   # input batch size for testing (default: 1000)\n",
    "VAL_EPOCHS = 1 \n",
    "LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n",
    "SEED = 42               # random seed (default: 42)\n",
    "MAX_LEN = 128           # Max length for product description\n",
    "SUMMARY_LEN = 20         # Max length for product names\n",
    "\n",
    "TRAIN_EPOCHS = 2       # number of epochs to train (default: 10)\n",
    "WEIGHT_DECAY = 0.01\n",
    "SEED = 42               # random seed (default: 42)\n",
    "MAX_LEN = 128\n",
    "SUMMARY_LEN = 20   # Maximum length of caption generated by the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fe6145",
   "metadata": {},
   "source": [
    "# Preparing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a581f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(r'D:\\Downloads\\ML\\Image_Captioning_using_Hugging_Face-main\\Image_Captioning_using_Hugging_Face-main')\n",
    "import json\n",
    "\n",
    "\n",
    "with open('data.json', 'r') as openfile:\n",
    "\n",
    "    json_object = json.load(openfile)\n",
    "\n",
    "images_caption_dict = dict(json_object)\n",
    "\n",
    "\n",
    "images_path = 'D:\\\\Downloads\\\\ML\\\\flickr8k\\\\Images'\n",
    "images = list(images_caption_dict.keys())\n",
    "\n",
    "for image_path in images:\n",
    "    if image_path.endswith('jpg'):\n",
    "        new = images_path + image_path.split('/')[-1]\n",
    "        images_caption_dict[new] = images_caption_dict.pop(image_path)\n",
    "    else:\n",
    "        images_caption_dict.pop(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "203d74b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([])\n",
    "\n",
    "captions = []\n",
    "images = []\n",
    "for image in list(images_caption_dict.keys()):\n",
    "    caption = images_caption_dict[image]\n",
    "#     captions.append(('.'.join([ sent.rstrip() for sent in ('.'.join(caption)).split('<e>.<s>')]))\\\n",
    "#                             .replace('<s> ','').replace('  <e>','.'))\n",
    "    for capt in caption:\n",
    "        captions.append(capt.replace('<s> ','').replace('  <e>','').strip())\n",
    "        images.append(image)\n",
    "        \n",
    "df['images'] = images\n",
    "df['captions'] = captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674ab7d3",
   "metadata": {},
   "source": [
    "# ROBERTA\n",
    "### Training the Decoder Model for Language Understanding and build Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b12a0c9",
   "metadata": {},
   "source": [
    "### Tokenizer\n",
    "#### Converting captions in to .txt file for training of the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e956c1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store values in a dataframe column (Series object) to files, one file per record\n",
    "os.mkdir(\"./text_split\")\n",
    "def column_to_files(column, prefix, txt_files_dir = \"./text_split\"):\n",
    "    # The prefix is a unique ID to avoid to overwrite a text file\n",
    "    i=prefix\n",
    "    #For every value in the df, with just one column\n",
    "    for row in column.to_list():\n",
    "      # Create the filename using the prefix ID\n",
    "        file_name = os.path.join(txt_files_dir, str(i)+'.txt')\n",
    "        try:\n",
    "            # Create the file and write the column text to it\n",
    "            f = open(file_name, 'wb')\n",
    "            f.write(row.encode('utf-8'))\n",
    "            f.close()\n",
    "        except Exception as e:  #catch exceptions(for eg. empty rows)\n",
    "            print(row, e) \n",
    "        i+=1\n",
    "    # Return the last ID\n",
    "    return i\n",
    "\n",
    "data = df[\"captions\"]\n",
    "# Removing the end of line character \\n\n",
    "data = data.replace(\"\\n\",\" \")\n",
    "# Set the ID to 0\n",
    "prefix=0\n",
    "# Create a file for every description value\n",
    "prefix = column_to_files(data, prefix)\n",
    "# Print the last ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55eaff",
   "metadata": {},
   "source": [
    "#### Training tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23b01bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.28 s\n",
      "Wall time: 4min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "paths = [str(x) for x in Path(\".\").glob(\"text_split/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer(lowercase=True)\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=10000, min_frequency=2,\n",
    "                show_progress=True,\n",
    "                special_tokens=[\n",
    "                                \"<s>\",\n",
    "                                \"<pad>\",\n",
    "                                \"<e>\",\n",
    "                                \"<unk>\",\n",
    "                                \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2599851c",
   "metadata": {},
   "source": [
    "#### Save Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "463597cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Byte_tokenizer\\\\vocab.json', 'Byte_tokenizer\\\\merges.txt']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "os.mkdir('Byte_tokenizer')\n",
    "tokenizer.save_model('Byte_tokenizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c98310",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "#### Intialization & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "617c6d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num parameters:  51206416\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig(\n",
    "    vocab_size=10000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "\n",
    "print('Num parameters: ',model.num_parameters())\n",
    "\n",
    "# Create the tokenizer from a trained one\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('Byte_tokenizer', max_len=MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8ae1ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "        \n",
    "        for example in df.values:\n",
    "            x=tokenizer.encode_plus(example, max_length = MAX_LEN, truncation=True, padding=True)\n",
    "            self.examples += [x.input_ids]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # We’ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8289594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and evaluation dataset\n",
    "train_dataset = CustomDataset(df['captions'][:38000], tokenizer)\n",
    "eval_dataset = CustomDataset(df['captions'][38000:], tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20893ec7",
   "metadata": {},
   "source": [
    "#### Batching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f6f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Define the Data Collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e9ba3",
   "metadata": {},
   "source": [
    "## Training the Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae2f832b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"RobertaMLM\"\n",
    "# Define the training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_folder,\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    num_train_epochs=TRAIN_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=VALID_BATCH_SIZE,\n",
    "    save_steps=8192,\n",
    "    #eval_steps=4096,\n",
    "    save_total_limit=1,\n",
    ")\n",
    "# Create the trainer for our model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    #prediction_loss_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc692d01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9fe382",
   "metadata": {},
   "source": [
    "#### Check Perplexity score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d01728b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 491/491 [00:05<00:00, 92.07it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 30.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed68da",
   "metadata": {},
   "source": [
    "### Saving tokenizer & Model to use in Encoder Decoder architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba4ad34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Byte_tokenizer\\\\tokenizer_config.json',\n",
       " 'Byte_tokenizer\\\\special_tokens_map.json',\n",
       " 'Byte_tokenizer\\\\vocab.json',\n",
       " 'Byte_tokenizer\\\\merges.txt',\n",
       " 'Byte_tokenizer\\\\added_tokens.json',\n",
       " 'Byte_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('Byte_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62f26ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a830a53d",
   "metadata": {},
   "source": [
    "# Evaluating Decoder(ROBERTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa2efa74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model= r'RobertaMLM',\n",
    "    tokenizer= 'Byte_tokenizer'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b3853b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.09009407460689545,\n",
       "  'token': 491,\n",
       "  'token_str': ' large',\n",
       "  'sequence': 'a girl going into a large building'},\n",
       " {'score': 0.05363229662179947,\n",
       "  'token': 377,\n",
       "  'token_str': ' red',\n",
       "  'sequence': 'a girl going into a red building'},\n",
       " {'score': 0.044735923409461975,\n",
       "  'token': 618,\n",
       "  'token_str': ' dirt',\n",
       "  'sequence': 'a girl going into a dirt building'},\n",
       " {'score': 0.03572113811969757,\n",
       "  'token': 475,\n",
       "  'token_str': ' small',\n",
       "  'sequence': 'a girl going into a small building'},\n",
       " {'score': 0.03269856795668602,\n",
       "  'token': 402,\n",
       "  'token_str': ' blue',\n",
       "  'sequence': 'a girl going into a blue building'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"a girl going into a <mask> building\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8099aff",
   "metadata": {},
   "source": [
    "## This Roberta Model will be used as Decoder in Our Image Captioning model and will be connnected to ViT Encoder model using cross attention heads."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
